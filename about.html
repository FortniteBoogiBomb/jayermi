<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About</title>
    <!-- Link to your CSS file -->
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Header section -->
    <header>
        <nav>
            <div class="container">
                <h1 class="logo">
                    <span class="gear-animation"></span>
                    <img src="images/JayS.png" alt="New Logo" class="new-logo">
                    <span class="gear-animation"></span>
                </h1>
                <ul class="menu">
                    <li><a href="index">Home</a></li>
                    <li><a href="contact.html">Contact</a></li>
                    <li><a href="extra.html">Extra</a></li>
                    <li><a href="https://github.com/FortniteBoogiBomb">GitHub</a></li>
                    <li><a href="https://www.linkedin.com/in/jay-ermi-ab1b95148/">Linkedin</a></li>
                    <!-- Add more navigation links as needed -->
                </ul>
            </div>
        </nav>
    </header>
    <!-- Main content section -->
    <main>
        <h2>About Page</h2>
        <p> Hi there! I'm Jay I am 23 years old and just graduated from Indiana University.</p>
        <p>I have a Bacholer of Science in Intellegent Systems Engineering, with a concentration in Computer Engineering.</p>
        <p>I have been programming with C & Python for 4+ years, and I have extensive experience with MATLAB and microcontrollers.</p>
        <p>I have a solid understanding of widely used libraries and frameworks like NumPy, pandas, and scikit-learn.</p>
        <p>Using pandas, I perform data manipulation, exploration, and feature engineering, while scikit-learn empowers me to build and train robust machine learning models.</p>
        <p>I excel in implementing various machine learning algorithms, including linear regression, decision trees, random forests, and support vector machines.</p>
        <p>My last internship with Reckitt provided me with proffesional data analytics and software engineering experince, using a company-wide product lifecycle mangement software.</p>
        <p>Addtionally, I possess a background in other programming languages, embedded systems, and robotics/drones</p>
        <div class="pdf-container">
            <object class="pdf-object" data="Jay Ermi Resume.pdf" type="application/pdf">
            </object>
        </div>
        <button class="download-resume-button"><a href="Jay Ermi Resume.pdf" download>Download Resume</a></button>
        <h2>Capstone</h2>
        <div class="media-item">
            <img src="images/team7.jpg" alt= "Capstone Team 7" class="my-image">
        </div>
        <p>One of the more signifigant projects that I worked on as an Engineer was my senior design capstone project.</p>
        <p>Our prototype was called the EMG controlled exo-skeltal arm brace and was sponsored by VispalExo.</p>
        <p>An arduino microcontroller controls the logic of the brace using a support vector machine learning algorithim.</p>
        <p>The SVM classifier is able to read the emg signal and determine whether the user is flecing or extending the attatched muscle. </p>
        <a href="https://vispalexo.com/">VispalExo Website</a> <!-- Hyperlink added here -->
        <!-- Add a wrapper div for the videos -->
        <div class="media-container">
            <div class="media-item">
                <img src="images/IMG_2949%20(1).jpg" alt="Picture of brace hardware, I personally designed & assembled this prototype myself" class="my-image">
                <p>hardware assembly of Exo-skeleton brace</p>
            </div>
            <div class="media-item">
                <video controls>
                    <source src="images/IMG_2667.MOV" type="video/mp4">
                    <!--Your browser does not support the video tag.-->
                </video>
                <p>First Demo of the exo-skeleton arm from first semester</p>
            </div>
            <div class="media-item">
                <video controls>
                    <source src="images/IMG_3167%20(4).MOV" type="video/mp4">
                    <!--Your browser does not support the video tag.-->
                </video>
                <p>Working implementation of EMG controlled brace (EMG sensor on left arm)</p>
            </div>
        </div>
        <h2>Undergraduate research</h2>
        <figure>
            <img src="images/trusted.jpg" alt="Trusted Ai Symposium Photo" />
            <figcaption>Photo from the trusted Ai symposium at Indiana University Center for artificial intellgence </figcaption>
        </figure>
        <p>During my time in the lab, I primarily worked on two studies. The first one I worked on </p>
        <p>was called Trusted Ai or Trust In Ai (depending on who you ask.) This project is funded by NSA Crane and is still active in the socio-physio lab. </p>
        <p>This study involved one outside subject and one lab member. The subject and the lab member were instructed to either </p>
        <p>play cooperativley to achieve a high score or play against each other to achieve a high score. The cool part about this game</p>
        <p>is that if you don't work together, you will almost always lose more money than you would working in tandem. The game consisted</p>
        <p>of two haptic controllers in seperate rooms. Each haptic device controlled its own agent, but each of the two agents were tethered together.</p>
        <p>The two players were also provided with a camera to visually comunicate with the other player, but they were physically prevented from
        <p>speaking to each other verbally. Once the game started the participants were shown the two agents and the tethter connecting them.
        <p>When one player moved their haptic controller up, the other player would feel the pull from their counterpart on their hand.</p>
        <p>To make things even spicier, the gates that were shown to each player were positioned similar to flappy bird (except there are two gates you could pass through).</p>
        <p>One gate would have a net income of money while the other would have a net loss. Failing to pass through either of the gates would have a radical impact on your money.</p>
        <p>So players were forced to play, knowing that the "opponent" would see a chance to gain more money or chose to cooperate and potenially lose money. </p>
        <p>However if each player were to pull to the opposing gate, they would both lose a ton of money (or all of it).</p>
        <p>This study attempted to evaluate the trust between people and an autonomous agent. The study had some interesting findings; one subject even got so mad they got up and left the lab!</p>
        <p>During the spring semester, we presented some of the findings to the Department of Defense. We were able to show them the protocol and some of the visitors even played our game. </p>
        <p>The main contribution I made towards this project was with the haptic controller code, bug fixing the code for the "hapty-bird" game,</p>
        <p>running subjects through the protocol, and gathering/visualizing data collected on subjects to analyze results and findings.</p>
        <p>Trusting Ai is sponsored by NSA Crane and is still currently accepting subjects. Please see these article's to learn more about Trusted Ai.</p>
        <li><a href="https://research.impact.iu.edu/key-areas/cyber-and-national-security/stories/trusted-ai.html?_gl=1*15nbrqm*_ga*NjI5NTM3MDcxLjE2ODc5NzM5OTA.*_ga_61CH0D2DQW*MTY4Nzk3Mzk5MC4xLjAuMTY4Nzk3Mzk5MC42MC4wLjA">Trusted AI Article 1</a></li>
        <li><a href="https://news.luddy.indiana.edu/story.html?story=Indiana-University-to-receive-44-million-for-Trusted-AI-research">Trusted Ai Article 2</a></li>
        <h3>Autisim Movement Study</h3>
        <p>During my research journey, I dedicated a significant amount of time to the Autism Movement study in collaboration with researchers from IU Kokomo.</p>
        <p>This study, conducted at the Socio Neural lab, focuses on understanding the movements and interpretations of individuals across the broad autism spectrum disorder.</p>
        <p>Within this project, we developed two main protocols aimed at investigating the emotional movements of participants.</p>
        <p>The first protocol involved utilizing a Kinect camera to track the movements of participants, capturing point line data.</p>
        <p>Participants were instructed to perform a series of "emotional" movements. These were classified by raters to determine the intensity of the emotion expressed.</p>
        <p>By analyzing the point line data using Python along with the SciPy library, we assessed the severity of movement in comparison to baseline</p>
        <p>actors, enabling us to gain insights into how individuals with autism spectrum disorder differ in their motor behavior.</p>
        <p>With the help of Python's image processing capabilities and the SciPy library, we processed the recorded video footage</p>
        <p>and extracted the coordinates of the body points for each frame.</p>
        <p>This involved utilizing computer vision algorithms to detect and track specific body joints or markers.</p>
        <p> Once the point line data was extracted and processed, we transformed it into a suitable format using the pandas library</p>
        <p>to export the point line data to Excel for further analysis and visualization.</p>
        <p>The second protocol involved participants sitting in front of a computer screen and following instructions provided in a video. They were</p>
        <p>shown two videos: the first one presented instructions, guiding the subjects to mirror emotions displayed screen as closely as possible.</p>
        <p>The second video consisted of a collection of baseline emotional responses.</p>
        <p>In this stage of the study, I played a key role in developing the protocol for the mirroring faces exercise.</p>
        <p>Additionally, I was responsible for processing the collected data from OpenFace and loading it into MATLAB for further analysis.</p>
        <p>Utilizing data processing channels such as OpenFace and OpenCV, we were able to analyze facial affect data and identify
        <p>significant differences between individuals with autism spectrum disorder and neurotypical subjects.</p>
        <p>By employing data analytics techniques, we mass-processed the data and extracted relevant features</p>
        <p>including the facial expressions that differed the most from the baseline.</p>
        <p>This groundbreaking research aims to quantify movement patterns as "Autistic" or "Neuro-Typical." </p>
        <p>It is important to note that the study is still in development, and the findings are yet to be published.</p>
        <p>Currently, our lab is actively seeking grant funding to further support this vital research in the field of mental health.</p>
        <li><a href="https://apps.iupuc.edu/news/2023/release/920_iupuc-professor-shares-latest-research-at-autism-research-conference">"Autisim Movement Study article"</a></li>
        <!-- Add more content as needed -->
        <h3>Engineering projects</h3>

    </main>

    <!-- Footer section -->
    <footer>
        <p>&copy; 2023 JaySpot. All rights reserved.</p>
        <span id="last-updated"></span>
    </footer>
</body>
</html>

